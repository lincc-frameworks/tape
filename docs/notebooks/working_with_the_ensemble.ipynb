{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the TAPE `Ensemble` object"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with many lightcurves, the TAPE `Ensemble` object serves as a singular interface for storing, filtering, and analyzing timeseries data. \n",
    "Let's consider an example set of lightcurves:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tape.ensemble import Ensemble\n",
    "\n",
    "ens = Ensemble()  # initialize an ensemble object\n",
    "\n",
    "# Read in data from a parquet file\n",
    "ens.from_parquet(\n",
    "    \"../../tests/tape_tests/data/source/test_source.parquet\",\n",
    "    id_col=\"ps1_objid\",\n",
    "    time_col=\"midPointTai\",\n",
    "    flux_col=\"psFlux\",\n",
    "    err_col=\"psFluxErr\",\n",
    "    band_col=\"filterName\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an `Ensemble` object, and have provided it with data from a parquet file. Within the call to `Ensemble.from_parquet`, we specified which columns of the input file mapped to timeseries quantities that the `Ensemble` needs to understand. It's important to link these arguments properly, as the `Ensemble` will use these columns when operations are requested on understood quantities. For example, if an TAPE analysis function requires the time column, from this linking the `Ensemble` will automatically supply that function with the 'midPointTai' column."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Mapping with the ColumnMapper\n",
    "\n",
    "In the above example, we manually provide the column labels within the call to `Ensemble.from_parquet`. Alternatively, the `tape.utils.ColumnMapper` class offers a means to assign the column mappings. Either manually as shown before, or even populated from a known mapping scheme. Since our dataset is from ZTF, we can utilize the known mapping for ZTF data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tape.utils import ColumnMapper\n",
    "\n",
    "# columns assigned manually\n",
    "manual_colmap = ColumnMapper().assign(\n",
    "    id_col=\"ps1_objid\", time_col=\"midPointTai\", flux_col=\"psFlux\", err_col=\"psFluxErr\", band_col=\"filterName\"\n",
    ")\n",
    "\n",
    "# columns assigned using known ZTF columns\n",
    "ztf_map = ColumnMapper().use_known_map(\"ZTF\")\n",
    "\n",
    "# Pass the ColumnMapper along to read_parquet\n",
    "ens.from_parquet(\"../../tests/tape_tests/data/source/test_source.parquet\", column_mapper=ztf_map)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Object and Source Frames\n",
    "The `Ensemble` maintains two dataframes under the hood, the \"object dataframe\" and the \"source dataframe\". This borrows from the Rubin Observatories object-source convention, where object denotes a given astronomical object and source is the collection of measurements of that object. Essentially, the Object frame stores one-off information about objects, and the source frame stores the available time-domain data. As a result, Ensemble functions that operate on the underlying dataframes need to be pointed at either object or source. In most cases, the default is the object table as it's a more helpful interface for understanding the contents of the ensemble, especially when dealing with large volumes of data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask and \"Lazy Evaluation\"\n",
    "\n",
    "Before going any further, the `Ensemble` is built on top of `Dask`, which brings with it a powerful framework for parallelization and scalability. However, there are some differences in how `Dask` code works that, if you're unfamiliar with it, is worth establishing right here at the get-go. The first is that `Dask` evaluates code \"lazily\". Meaning that many operations are not executed when the line of code is run, but instead are added to a scheduler to be executed when the result is actually needed. See below for an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens._source  # We have not actually loaded any data into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are accessing the Dask dataframe underneath, and despite running a command to read in our data, we only see an empty dataframe with some high-level information available. To explicitly bring the data into memory, we must run a `compute()` command."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.compute(\"source\")  # Compute lets dask know we're ready to bring the data into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this compute, we see above that we now have a populated dataframe (a Pandas dataframe in fact!). From this, many workflows in Dask and by extension TAPE, will look like a series of lazily evaluated commands that are chained together and then executed with a .compute() call at the end of the workflow.\n",
    "\n",
    "Alternatively we can use `ens.persist()` to execute the chained commands without loading the result into memory. This can speed up future `compute()` calls."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection, Filtering, and Selecting\n",
    "\n",
    "The `Ensemble` contains an assortment of functions for inspecting and filtering your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspection\n",
    "\n",
    "These functions provide views into the contents of your `Ensemble` dataframe, especially important when dealing with large data volumes that cannot be brought into memory all at once. The first is `Ensemble.info` which provides information on the columns, data types, and memory usage of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Inspection\n",
    "\n",
    "ens.info(verbose=True, memory_usage=True)  # Grabs high level information about the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Ensemble.info` shows that we have 2000 rows with 54.7 KBs of used memory, and shows the columns we've brought in with their respective data types. If you'd like to actually bring a few rows into memory to inspect, `Ensemble.head` and `Ensemble.tail` provide access to the first n and last n rows respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.head(\"object\", 5)  # Grabs the first 5 rows of the object table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.tail(\"source\", 5)  # Grabs the last 5 rows of the source table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, when you are working with a small enough dataset, `Ensemble.compute` can be used to bring the whole dataframe into memory (as shown previously). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.compute(\"source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "The `Ensemble` provides a general filtering function `query` that mirrors a Pandas or Dask `query` command. Specifically, the function takes a string that provides an expression indicating which rows to **keep**. As with other `Ensemble` functions, an optional `table` parameter allows you to filter on either the object or the source table.\n",
    "\n",
    "For example, the following code filters the sources to only include rows with a flux value above 18.2. It uses `ens._flux_col` to retrieve the name of the column with that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.query(f\"{ens._flux_col} > 18.2\", table=\"source\")\n",
    "ens.compute(\"source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use a Dask dataseries of Booleans to indicate which rows to *keep*. We can often compute these series as the result of some operation on the underlying tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "keep_rows = ens._source[ens._err_col] < 0.05\n",
    "keep_rows.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We then pass that series to a `filter_from_series` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.filter_from_series(keep_rows, table=\"source\")\n",
    "ens.compute(\"source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, several more specific functions are available for common operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cleaning nans\n",
    "ens.dropna(threshold=1)  # threshold is the number of nans present in a row needed to drop the row\n",
    "\n",
    "# Filtering on number of observations\n",
    "ens.prune(threshold=50)  # threshold is the number of observations needed to retain the object\n",
    "\n",
    "ens.info(verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above operations, we remove any rows that have at least 1 NaN value present. And then filter such that only lightcurves which have at least 50 measurements are retained. We see then that `Ensemble.info` shows we've reduced the size of the dataset from 2000 rows to 1928 rows."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting\n",
    "\n",
    "The `Ensemble` also provides a `select` function to filter down to a subset of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add a new column so we can filter it out later.\n",
    "ens._source = ens._source.assign(band2=ens._source[\"filterName\"] + \"2\")\n",
    "ens.compute(\"source\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.select([\"midPointTai\", \"psFlux\", \"psFluxErr\", \"filterName\"], table=\"source\")\n",
    "ens.compute(\"source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments and Column Manipulation\n",
    "\n",
    "The ensemble object supports assignment through the Dask `assign` function. We can pass in either a callable or a series to assign to the new column. New column names are produced automatically from the argument name.\n",
    "\n",
    "For example, if we want to compute the lower bound of an error range as the estimated flux minus twice the estimated error, we would use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.assign(table=\"source\", lower_bnd=lambda x: x[\"psFlux\"] - 2.0 * x[\"psFluxErr\"])\n",
    "ens.compute(table=\"source\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Analysis\n",
    "\n",
    "The `Ensemble` provides a powerful batching interface, `Ensemble.batch`, with in-built parallelization (provided the input data is in multiple partitions). In addition, TAPE has a suite of analysis functions on-hand for your use. Below, we show the application of `tape.analysis.calc_stetson_J` on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using tape analysis functions\n",
    "from tape.analysis import calc_stetson_J, stetson_J_required_columns\n",
    "\n",
    "# Extract the mapping of columns to name strings.\n",
    "column_map = ens.make_column_map()\n",
    "\n",
    "# compute is set to true to execute immediately (non-lazily)\n",
    "res = ens.batch(calc_stetson_J, *stetson_J_required_columns(column_map), compute=True)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Custom Analysis Function\n",
    "The analysis functions contained in TAPE are meant to provide a collection of performant, on-hand routines for common timeseries use cases. However, TAPE is also equipped to handle externally defined functions. Let's walk through a short example of defining a simple custom function and applying it through `Ensemble.batch`.\n",
    "\n",
    "Here we define a simple function, that returns an average flux for each photometric band. It requires an array of fluxes, an array of band labels per measurement, and has a keyword argument for determining which averaging strategy to use (mean or median)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Defining a simple function\n",
    "def my_flux_average(flux_array, band_array, method=\"mean\"):\n",
    "    \"\"\"Read in an array of fluxes, and return the average of the fluxes by band\"\"\"\n",
    "    res = {}\n",
    "    for band in np.unique(band_array):\n",
    "        mask = [band_array == band]  # Create a band by band mask\n",
    "        band_fluxes = flux_array[tuple(mask)]  # Mask the flux array\n",
    "        if method == \"mean\":\n",
    "            res[band] = np.mean(band_fluxes)\n",
    "        elif method == \"median\":\n",
    "            res[band] = np.median(band_fluxes)\n",
    "    return res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function defined, we next supply it to `Ensemble.batch`. The column labels of the `Ensemble` columns we want to use as arguments must be provided, as well as any keyword arguments. In this case, we pass along `\"psFlux\"` and `\"filterName\"`, so that the `Ensemble` will map those columns to `flux_array` and `band_array` respectively. We also pass `method='mean'`, which will pass that kwarg along to `my_flux_average`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Applying the function to the ensemble\n",
    "res = ens.batch(my_flux_average, \"psFlux\", \"filterName\", compute=True, meta=None, method=\"median\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we now have a `Pandas.series` of `my_average_flux` result by object_id (lightcurve). In many cases, this may not be the ideal output for your function. This output is controlled by the `Dask` `meta` parameter. For more information on this parameter, you can read the `Dask` [documentation](https://blog.dask.org/2022/08/09/understanding-meta-keyword-argument). You may pass the `meta` parameter through `Ensemble.batch`, as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.client.close()  # Tear down the ensemble client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "08968836a6367873274ed1d5e98a07391f42fc3a62bd5aba54afbd7b11ba8673"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
