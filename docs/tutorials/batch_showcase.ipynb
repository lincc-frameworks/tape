{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ensemble Batch Showcase\n",
    "\n",
    "`Ensemble.batch` is a versatile function that allows users to pass in external functions that operate on groupings of `Ensemble` data, most commonly these are functions that calculate something per lightcurve. Because external functions can have a huge variety of inputs and outputs, this notebook serves as a collection of example functions and how `batch` can be used with them. The hope is that there is a function here similar to a function that you are trying to apply via `batch` so that example can be used as a template for getting your function to work."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Generate some toy data and create an Ensemble"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tape import Ensemble, ColumnMapper, TapeFrame\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import sys"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate some fake data\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "obj_ids = []\n",
    "mjds = []\n",
    "for i in range(10,110):\n",
    "    obj_ids.append(np.array([i]*1250))\n",
    "    mjds.append(np.arange(0.,1250.,1.))\n",
    "obj_ids = np.concatenate(obj_ids)\n",
    "mjds = np.concatenate(mjds)\n",
    "\n",
    "flux = 10*np.random.random(125000)\n",
    "err = flux/10\n",
    "band = np.random.choice(['g','r'], 125000)\n",
    "\n",
    "source_dict = {\"id\":obj_ids, \"mjd\":mjds,\"flux\":flux,\"err\":err,\"band\":band}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the data into an Ensemble\n",
    "ens = Ensemble()\n",
    "\n",
    "ens.from_source_dict(source_dict, column_mapper = ColumnMapper(id_col=\"id\",\n",
    "                                                              time_col=\"mjd\",\n",
    "                                                              flux_col=\"flux\",\n",
    "                                                              err_col=\"err\",\n",
    "                                                              band_col=\"band\"))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 1: A Simple Mean"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We define a simple function that takes in an array-like argument, `flux`, and returns it's mean."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1: Simple\n",
    "def my_mean(flux):\n",
    "    return np.mean(flux)\n",
    "\n",
    "my_mean([1,2,3,4,5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To run the `my_mean` function with `Ensemble.batch`, we simply pass the function, and the argument(s) as separate function arguments. In this case, we pass \"flux\" as a string, as batch will grab the data at that column label to evaluate on."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default batch\n",
    "res1 = ens.batch(my_mean, \"flux\") # \"flux\" is provided to have TAPE pass the \"flux\" column data along to my_mean\n",
    "res1.compute() # Compute to see the result"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default, `Ensemble.batch` groups each lightcurve together (grouping on the specified id column). However, batch also support custom grouping assignments, as below we instead pass `on=[\"band\"]`, letting batch know to calculate the mean for all data from each band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch with custom grouping\n",
    "\n",
    "res2 = ens.batch(my_mean, \"flux\", on=[\"band\"])\n",
    "res2.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This can be extended to more than just a single column, as below we group by id and then sub-group by band. In `Pandas`, an operation like this would return a multi-index, but due to `Dask` not supporting multi-indexes we return sub-groupings as columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-level groupbys\n",
    "\n",
    "res3 = ens.batch(my_mean, \"flux\", on=[\"id\", \"band\"])\n",
    "res3.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sub-grouping by photometric band is a use case we expect to be common in TAPE workflows, and so there is the `by_band` kwarg available within batch. This will ensure that the last sub-grouping level is on band and will return independent columns for each band result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch with the by_band flag\n",
    "res4 = ens.batch(my_mean, \"flux\", by_band=True)\n",
    "res4.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 2: Functions That Return a Series"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In case 2, we write a function that returns a `Pandas.Series` object. This object has the min and max of the flux array stored at different indices of the output series."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_bounds(flux):\n",
    "    return pd.Series({\"min\":np.min(flux), \"max\":np.max(flux)})\n",
    "\n",
    "# Function output\n",
    "my_bounds([1,2,3,4,5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As in case 1, we're able to pass this function and the \"flux\" column along to run the function. However, this time we need the `meta` to be set. The `meta` is a needed component of `Dask's` lazy evaluation. As `Dask` does not actually compute results until requested to, `meta` serves as the expected form of the output. In this case, we just need to let `Dask` know that a min and max column will be present in a dataframe (TAPE will always return a dataframe)  and that both will be float values.\n",
    "\n",
    "For more information on the `Dask` meta argument, read their [documentation](https://blog.dask.org/2022/08/09/understanding-meta-keyword-argument)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Batch\n",
    "\n",
    "res1 = ens.batch(my_bounds, \"flux\", meta={\"min\":float, \"max\":float}) # Requires meta to be set\n",
    "res1.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The same flexibility with grouping extends to case 2, with again needing to specify the `meta`. Note that the meta given to `Ensemble.batch` remains the same, only depending on the function output, it handles the meta for any columns generated by the grouping on it's own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Multi-level groupbys, note that meta does not need to change\n",
    "res2 = ens.batch(my_bounds, \"flux\", on=[\"id\", \"band\"], meta={\"min\":float, \"max\":float}) # Requires meta to be set\n",
    "res2.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using the `by_band` kwarg extends the output columns to be per-band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Using by_band\n",
    "\n",
    "res3 = ens.batch(my_bounds, \"flux\", by_band=True, meta={\"min\":float, \"max\":float}) # Requires meta to be set\n",
    "res3.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 3: Functions That Return a DataFrame"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we define a function, `my_bounds_df` that computes the same quantities as `my_bounds` above, but in this case we return a dataframe of the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def my_bounds_df(flux):\n",
    "    return pd.DataFrame({'min':[np.min(flux)], 'max':[np.max(flux)]})\n",
    "\n",
    "my_bounds_df([1,2,3,4,5])"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is perfectly reasonable, but when passing a function like this through `batch` there's an issue currently to watch out for."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Default Batch, some things to watch out for\n",
    "\n",
    "res1 = ens.batch(my_bounds_df, \"flux\", meta={'min':float, 'max':float})\n",
    "res1.compute()\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the series, we needed to pass the `meta` kwarg letting TAPE know which output columns to expect from the function. However,\n",
    "we see that our result is carrying over the index generated by the dataframe in addition to the batch index, represented as a multi-index. At the time of this notebooks creation, `Dask` does not have explicit support for multi-indexes. We can see this problem in the following cells."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pandas resolves these indexes as a multi-index\n",
    "res1.reset_index().compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dask assumes there's just a single index column being sent to the dataframe columns\n",
    "res1.reset_index()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When `Dask` and the underlying `Pandas` disagree on what the dataframe looks like, this causes issues with you as the user being able to work with the dataframe. As `Dask` won't recognize any calls to \"id\" or \"level_1\" here, and instead will only accept a call to \"index\" which in turn `Pandas` won't understand. If this is the issue you run into, we recommend trying to modify your function into a non-dataframe output format. However, in the case that this isn't possible, here's a somewhat hacky way to move around it.\n",
    "\n",
    "We can resolve this by updating the `Dask` meta manually, to re-align `Dask` and `Pandas`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If it's not too compute intensive, grabbing the actual dataframe is the easiest way forward\n",
    "real_meta_from_result = res1.reset_index().head(0)\n",
    "real_meta_from_result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# otherwise, can generate this ourselves\n",
    "real_meta_from_dataframe = TapeFrame(columns=[\"id\",\"level_1\",\"min\",\"max\"])\n",
    "real_meta_from_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Overwrite the _meta property\n",
    "\n",
    "res1_noindex = res1.reset_index()\n",
    "res1_noindex._meta = real_meta_from_dataframe\n",
    "res1_noindex"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that in the above, we've reset the index as `Dask` will not support meta that tracks a multi-index. In the case of this function, we gain no information from the \"level_1\" column, and it would be nice to restablish \"id\" as the index, so we close the loop by executing the commands in the next cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res1 = res1_noindex.drop(columns=[\"level_1\"]).set_index(\"id\")\n",
    "res1.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 4: Functions that Require Non-Array Inputs"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's return to case 1, but this time instead of the list-like `flux` argument, let's say that the function needs to take in a dataframe with a column titled `my_flux`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 4: DataFrame input\n",
    "def my_mean_from_df(df):\n",
    "    return np.mean(df['my_flux'])\n",
    "\n",
    "df = pd.DataFrame({'my_flux':[1, 2, 3, 4, 5]})\n",
    "my_mean_from_df(df)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, batch won't be able to directly provide inputs to this function, as batch passes along the column data as arrays to the function. However, we can make this function able to be used by batch by wrapping it with another function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def mean_wrapper(flux):\n",
    "    df = pd.DataFrame({'my_flux': flux})\n",
    "    return my_mean_from_df(df)\n",
    "\n",
    "# Can pass the wrapper function along to batch\n",
    "res1 = ens.batch(mean_wrapper, \"flux\")\n",
    "res1.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This is a really simple case, but highlights that in some cases a wrapper function can be written to serve as a middle man between your function and `batch`, even doing work to sort or filter your data on a per function call basis if not done as a pre-filter step for your Ensemble."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Case 5: TAPE Analysis Functions"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TAPE analysis functions are a special case of input function to `Ensemble.batch`, where normally required information such as the specified column labels to pass to the function and the `meta` are passed along from the function to `Ensemble.batch` internally, meaning you just need to specify the function and any additional kwargs. For this case, let's leverage the [light-curve](https://github.com/light-curve/light-curve-python) package, which implements the extraction of many light curve [features](https://github.com/light-curve/light-curve-python?tab=readme-ov-file#available-features) used in astrophysics. Feature extraction from this package is also supported within TAPE as an analysis function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Grab two features extraction methods from light-curve\n",
    "from light_curve import Periodogram, OtsuSplit"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the below example, we apply the Lomb-Scargle Periodogram to our `Ensemble` light curves. Again, noting that in this case the `meta` we had to configure above is already handled by TAPE, and the needed timeseries columns are already passed along internally as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find periods using Lomb-Scargle periodogram\n",
    "periodogram = Periodogram(peaks=1, nyquist=0.1, max_freq_factor=10, fast=False)\n",
    "\n",
    "# Use r band only\n",
    "res_per = ens.batch(periodogram, band_to_calc='r') # band_to_calc is a kwarg of Periodogram\n",
    "res_per.compute()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, we use the `OtsuSplit` function, used to perform automatic thresholding. In this case, we also supply the `by_band` kwarg to get a result per photometric band."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_otsu = ens.batch(OtsuSplit(), band_to_calc=None, by_band=True)\n",
    "res_otsu.compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "83afbb17b435d9bf8b0d0042367da76f26510da1c5781f0ff6e6c518eab621ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
