{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rel_path = \"../../tests/tape_tests/data\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Access with TAPE\n",
    "\n",
    "This tutorial demonstrates the various ways in which data can be ingest into TAPE."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from Parquet Files\n",
    "\n",
    "The [Apache Parquet](https://parquet.apache.org/docs/overview/) format is an efficient column-oriented data file format well-suited for bulk datasets. TAPE provides functionality to create an `Ensemble` object from input parquet files via the `Ensemble.from_parquet()` function. At minimum, a parquet file containing time series information needed to populate the `Ensemble` source table should be supplied, as shown below. The `Ensemble` object table is created dynamically from the source table in this instance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tape.ensemble import Ensemble\n",
    "from tape.utils import ColumnMapper\n",
    "\n",
    "ens = Ensemble()  # initialize an ensemble object\n",
    "\n",
    "# A ColumnMapper is created to map columns of the parquet file to timeseries quantities, such as flux, time, etc.\n",
    "col_map = ColumnMapper(\n",
    "    id_col=\"ps1_objid\", time_col=\"midPointTai\", flux_col=\"psFlux\", err_col=\"psFluxErr\", band_col=\"filterName\"\n",
    ")\n",
    "\n",
    "# Read in data from a parquet file that contains source (timeseries) data\n",
    "ens.from_parquet(source_file=f\"{rel_path}/source/test_source.parquet\", column_mapper=col_map)\n",
    "\n",
    "ens.source.head(5)  # View the first 5 entries of the source table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, if object level information is available in a second parquet file, that may also be provided to populate the `Ensemble` object table, as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens = Ensemble()  # initialize an ensemble object\n",
    "\n",
    "col_map = ColumnMapper(\n",
    "    id_col=\"ps1_objid\",\n",
    "    time_col=\"midPointTai\",\n",
    "    flux_col=\"psFlux\",\n",
    "    err_col=\"psFluxErr\",\n",
    "    band_col=\"filterName\",\n",
    ")\n",
    "\n",
    "# Read in data from a parquet file that contains source (timeseries) data\n",
    "ens.from_parquet(\n",
    "    source_file=f\"{rel_path}/source/test_source.parquet\",\n",
    "    object_file=f\"{rel_path}/object/test_object.parquet\",\n",
    "    column_mapper=col_map,\n",
    ")\n",
    "\n",
    "ens.object.head(5)  # View the first 5 entries of the object table"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above examples, we use the `ColumnMapper` helper class to facilitate mapping of parquet file columns to a set of internally recognized quantities, such as flux, time, ids, errors, etc. These quantities are used to infer the correct columns to use when applying certain filtering operations, or when using `TAPE.analysis` functions. It may be the case that you aren't sure what columns are actually present in a given parquet file before attempting to ingest into TAPE. In these instances, we recommend using the pyarrow package to preview metadata, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyarrow import parquet\n",
    "\n",
    "parquet.read_schema(f\"{rel_path}/source/test_source.parquet\", memory_map=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Apache parquet files have many advantages for the type of scalable workflows that TAPE seeks to enable. A key advantage being that the parquet file supports in-format partitioning of large datasets. TAPE, by virtue of using `Dask`, inherits these partitions on load, avoiding any need to manually set a partitioning scheme for your data."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TAPE Datasets\n",
    "\n",
    "TAPE hosts a number of datasets that are retrievable by the user. These datasets have been added to demonstrate and test the various scientific workflows that TAPE has been developed to support. The `Ensemble.available_datasets()` may be used to see which datasets are available to retrieve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens = Ensemble()\n",
    "\n",
    "ens.available_datasets()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The function returns a dictionary of datasets and a brief description of their contents. To retrieve them, use the `Ensemble.from_dataset()` function with the dictionary key value corresponding to a specific dataset. Column mapping information is automatically generated for these known datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.from_dataset(\"s82_rrlyrae\")  # Let's grab the Stripe 82 RR Lyrae\n",
    "\n",
    "ens.object.head(5)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Loading from Array Data\n",
    "\n",
    "If your data is stored in arrays, `Ensemble.from_source_dict()` offers an interface to load these into an `Ensemble` object using a dictionary.\n",
    "\n",
    "Let's start by creating some example data arrays in a dictionary:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "np.random.seed(1)\n",
    "\n",
    "# initialize a dictionary of empty arrays\n",
    "source_dict = {\n",
    "    \"id\": np.array([]),\n",
    "    \"time\": np.array([]),\n",
    "    \"flux\": np.array([]),\n",
    "    \"error\": np.array([]),\n",
    "    \"band\": np.array([]),\n",
    "}\n",
    "\n",
    "# Create 10 lightcurves with 100 measurements each\n",
    "lc_len = 100\n",
    "for i in range(10):\n",
    "    source_dict[\"id\"] = np.append(source_dict[\"id\"], np.array([i] * lc_len)).astype(int)\n",
    "    source_dict[\"time\"] = np.append(source_dict[\"time\"], np.linspace(1, lc_len, lc_len))\n",
    "    source_dict[\"flux\"] = np.append(source_dict[\"flux\"], 100 + 50 * np.random.rand(lc_len))\n",
    "    source_dict[\"error\"] = np.append(source_dict[\"error\"], 10 + 5 * np.random.rand(lc_len))\n",
    "    source_dict[\"band\"] = np.append(source_dict[\"band\"], [\"g\"] * 50 + [\"r\"] * 50)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From here, we just need to pass the dictionary along to `Ensemble.from_source_dict()` and set the `ColumnMapper` appropriately."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "colmap = ColumnMapper(id_col=\"id\", time_col=\"time\", flux_col=\"flux\", err_col=\"error\", band_col=\"band\")\n",
    "ens = Ensemble()\n",
    "ens.from_source_dict(source_dict, column_mapper=colmap)\n",
    "\n",
    "ens.info()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "vscode": {
   "interpreter": {
    "hash": "83afbb17b435d9bf8b0d0042367da76f26510da1c5781f0ff6e6c518eab621ec"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
