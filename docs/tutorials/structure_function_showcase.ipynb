{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Structure Function Science Showcase"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tape\n",
    "from tape import Ensemble\n",
    "from tape.utils import ColumnMapper\n",
    "from tape.analysis.structure_function.base_argument_container import StructureFunctionArgumentContainer\n",
    "import eztao\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Structure function is often used to quantify the fluctuations or variations in a signal as a function of spatial or temporal separation. It is a method to analyze and describe the statistical properties of fluctuations within a dataset. In this tutorial, we will show how to use and employ structure function calculation as written in TAPE."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, before we get to the structure function calculations, we want to create the data with known\n",
    "temporal properties (and known structure function) so that we can verify our results. We will use the package `EzTao`, which enables us\n",
    "to create lightcurves with known variability.\n",
    "We will generate lightcurves with the process of damped random walk (represenatitive of typical AGN variability). Damped random walk differs from a standard random walk process \n",
    "by having an additional term that does not allow the process to venture too far away from its mean.\n",
    " \n",
    "These lightcurves created in this fashion fully described with two parameters - amplitude and decorrelation scale.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from eztao.carma import DRW_term\n",
    "from eztao.ts import gpSimRand\n",
    "\n",
    "# amp is the amplitude of the process and tau is the decorrelation timescale\n",
    "amp = 0.2\n",
    "tau = 100\n",
    "\n",
    "# Create the kernel\n",
    "DRW_kernel = DRW_term(np.log(amp), np.log(tau))\n",
    "\n",
    "# specify how many lightcurves and observational details\n",
    "num_light_curves = 100\n",
    "snr = 10\n",
    "duration_in_days = 365 * 10\n",
    "num_observations = 200\n",
    "\n",
    "# Generate `num_light_curves` lightcurves\n",
    "# t, y, yerr are np.ndarray with shape = (num_light_curves, num_observations)\n",
    "t, y, err = gpSimRand(\n",
    "    carmaTerm=DRW_kernel,\n",
    "    SNR=snr,\n",
    "    duration=duration_in_days,\n",
    "    N=num_observations,\n",
    "    nLC=num_light_curves)\n",
    "\n",
    "# pick 10 lightcurves at random and plot (with small offset for clarity)\n",
    "sel_10 = np.random.choice(len(t), size=10, replace=False)\n",
    "plt.figure(dpi=150, figsize=(10,5))\n",
    "for i, j in enumerate(sel_10):\n",
    "    plt.errorbar(t[j], y[j]+i*0.2, yerr=err[j], ls='', marker='.', ms=4, capsize=2)\n",
    "plt.xlabel('time [unit]')\n",
    "plt.ylabel('magnitude [unit]')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now calculate the structure function for these 10 lightcurves independently. \n",
    "This can be achieved by direct use of `calc_sf2` function, which takes\n",
    "times of the measurements, values of the measurements and the errors.\n",
    "The default method for calculating structure functions is to compute the variance of all of the data pairs\n",
    "in the time bin and subtract the squared error of the measurements, i.e.,\n",
    "$$\n",
    "SF^2 = \\frac{1}{P} \\sum_{i,j>i}^{P} (m_i - m_j)^2 - \\sigma_i^2-\\sigma_j^2\n",
    "$$\n",
    "where the sum is over all $P$ measurements which are separate by a time lag $\\Delta t$, $m$ are the values of\n",
    "measurements, and we are taking into account the measurement errors $\\sigma$ of the individual data points \n",
    "(See Press+ 1992, or [Caplar+ 2017]( \n",
    "10.3847/1538-4357/834/2/111) Equation 1, or [Kozlowski 2017](10.3847/0004-637X/826/2/118) Equation 12).\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We show here structure function for the same 10 lightcurves\n",
    "plt.figure()\n",
    "for i, j in enumerate(sel_10):\n",
    "    res_basic = tape.analysis.calc_sf2(t[j], y[j], err[j])\n",
    "    plt.plot(res_basic['dt'], res_basic['sf2'], marker='.',)\n",
    "\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.title(\"Structure Function^2\")\n",
    "plt.xlabel('Time [time unit]')\n",
    "plt.ylabel('SF^2 [magnitude unit^2]')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Working with Ensemble "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Even though all of the lightcurves were generated with the same underlying process, we see that\n",
    "there are differences, due to the stochastic nature of the process. \n",
    "The power of structure function calculation is that it can operate on\n",
    "multiple lightcurves at once, even if the lightcurves have different cadence of observations\n",
    "and different measurement uncertainties!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In order to show how this can be achieved, we will first put all of the 100 lightcurves generated\n",
    "in a TAPE `ensemble`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ens, t_ens, y_ens, yerr_ens, filter_ens\\\n",
    "    = np.array([]), np.array([]), np.array([]), np.array([]), np.array([])\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "for j in range(num_light_curves):\n",
    "    id_ens = np.append(id_ens, np.full(num_observations, j))\n",
    "    t_ens = np.append(t_ens, t[j])\n",
    "    y_ens = np.append(y_ens, y[j])\n",
    "    yerr_ens = np.append(yerr_ens, err[j])\n",
    "    filter_ens = np.append(filter_ens, np.full(num_observations, 'r'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "id_ens, t_ens, y_ens, yerr_ens, filter_ens\\\n",
    "    = [], [], [], [], []\n",
    "\n",
    "for j in range(num_light_curves):\n",
    "    id_ens.append(np.full(num_observations, j))\n",
    "    t_ens.append(t[j])\n",
    "    y_ens.append(y[j])\n",
    "    yerr_ens.append( err[j])\n",
    "    filter_ens.append(np.full(num_observations, 'r'))\n",
    "\n",
    "id_ens = np.array(id_ens)\n",
    "t_ens = np.array(t_ens)\n",
    "y_ens = np.array(y_ens)\n",
    "yerr_ens = np.array(yerr_ens)\n",
    "filter_ens = np.array(filter_ens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# First, we create all the columns that we will want to fill\n",
    "# In addition to time, measurement and errors, this includes \n",
    "# id for each lightcurve and information about the filter used\n",
    "# For the purpose of this tutorial let's assume we are observing in `r` filter\n",
    "\n",
    "id_ens, t_ens, y_ens, yerr_ens, filter_ens\\\n",
    "    = [], [], [], [], []\n",
    "\n",
    "for j in range(num_light_curves):\n",
    "    id_ens.append(np.full(num_observations, j))\n",
    "    t_ens.append(t[j])\n",
    "    y_ens.append(y[j])\n",
    "    yerr_ens.append( err[j])\n",
    "    filter_ens.append(np.full(num_observations, 'r'))\n",
    "\n",
    "id_ens = np.concatenate(id_ens)\n",
    "t_ens = np.concatenate(t_ens)\n",
    "y_ens = np.concatenate(y_ens)\n",
    "yerr_ens = np.concatenate(yerr_ens)\n",
    "filter_ens = np.concatenate(filter_ens)\n",
    "\n",
    "# This line makes sure that ids are integers      \n",
    "id_ens = id_ens.astype(int)\n",
    "\n",
    "# Create ColumnMapper object that will tell ensemble\n",
    "# meaning of each column\n",
    "manual_colmap = ColumnMapper().assign(\n",
    "    id_col=\"id_ens\", time_col=\"t_ens\", flux_col=\"y_ens\",\n",
    "    err_col=\"yerr_ens\", band_col=\"filter_ens\"\n",
    ")\n",
    "\n",
    "ens = Ensemble()\n",
    "ens.from_source_dict({'id_ens': id_ens, \"t_ens\": t_ens, 'y_ens': y_ens,\n",
    "                      'yerr_ens': yerr_ens, 'filter_ens': filter_ens},\n",
    "                        column_mapper=manual_colmap)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `ensemble` has an `object` table, capturing the information about the global properties of each \n",
    "lightcurve, while the actual observations are stored in the `source` table. In this case, our object table\n",
    "is empty, as no such information is provided. More information is available in the \n",
    "`Working with the TAPE Ensemble object` tutorial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.object.head(5)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.source.head(5)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To do more complex structure function calculations, we provide a powerful interface where users can extensively customize their calculations. This is done by instantiating \n",
    "`StructureFunctionArgumentContainer` and the specifying values for available options. \n",
    "\n",
    "Below, we show an example where we \n",
    "1. Specify that we want the calculation to be `combined', i.e., gather all of the data-pairs from different\n",
    "lightcurves together before calculating excess variance\n",
    "2. Specify the number of data pairs in a single time-bin (100000).\n",
    "3. Specify that we wish to estimate error. This is done by bootstrapping a sample of data pairs and estimating\n",
    "the 16th and 84th quantiles of the distribution.\n",
    "4. Specify how many bootstrapping resamples to do (100). This is also roughly the factor by which the calculation \n",
    "will slow down - because resampling takes the most computation time here. On the other hand, the estimate of\n",
    "the errors will be more accurate with more resampling. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "arg_container = StructureFunctionArgumentContainer()\n",
    "arg_container.combine = True\n",
    "arg_container.bin_count_target = 100000\n",
    "arg_container.estimate_err = True\n",
    "arg_container.calculation_repetitions = 100\n",
    "res_sf2 = ens.sf2(argument_container = arg_container)\n",
    "\n",
    "# We can also operate on a single lightcurve, without going via ensemble,\n",
    "# but with full amount of options going via argument container\n",
    "j = 7 # random lightcurve \n",
    "arg_container.combine = False # do not combine - only one lightcurve\n",
    "# only one lightcurve instead of 100 - specify 100 smaller count to get similar time-bin size as in\n",
    "# the full ensemble case\n",
    "arg_container.bin_count_target = 1000 \n",
    "\n",
    "\n",
    "res_one = tape.analysis.calc_sf2(t[j], y[j], err[j], argument_container = arg_container)\n",
    "\n",
    "# create error array here\n",
    "error_array = np.zeros(len(y[j]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To compare with empirical results we create theoretical structure function of the underlying model\n",
    "t_th = np.arange(0,10000,1)\n",
    "SF_th = (amp**2 * 2) * (1-np.exp(-t_th/tau))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_sf2.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res_one.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure()\n",
    "plt.errorbar(res_sf2['dt'], res_sf2['sf2'], yerr=res_sf2['1_sigma'],\n",
    "             marker='.', capsize=2, label='ensemble result', color='red')\n",
    "plt.errorbar(res_one['dt'], res_one['sf2'], yerr=res_one['1_sigma'],\n",
    "             marker='.', capsize=2, label='single lightcurve result', color='orange')\n",
    "plt.plot(t_th, SF_th, marker='', label='theoretical result', \n",
    "         color='black', lw=2)\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.title(\"Ensemble and single structure function\")\n",
    "plt.xlabel('Time [time unit]')\n",
    "plt.ylabel('SF^2 [magnitude unit^2]')\n",
    "plt.ylim(10**(-2), 10**(-0.4))\n",
    "plt.xlim(10**(1), 10**(3.8))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us discuss what is the meaning of these error bars. They are calculated from bootstrapping the sample, \n",
    "and as such, they should capture the inherent uncertainty of the stochastic process. In order to be less sensitive to the outliers, they are calculated by comparing the 84th and 16th quantiles of the distribution\n",
    "of generated structure function results. The code below illustrates the procedure."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# For the same lightcurve as above, let us resample it 100 times and put the results in separate array\n",
    "arg_container.estimate_err = False\n",
    "res_resample_arr = np.zeros((100,20))\n",
    "for i in range(100):  \n",
    "    resample_arr = np.sort(np.random.choice(np.arange(0,200), size=len(y[j]), replace=True))\n",
    "    res_resample = tape.analysis.calc_sf2(t[j][resample_arr], y[j][resample_arr], err[j][resample_arr],\n",
    "                                          argument_container=arg_container)\n",
    "    res_resample_arr[i]=res_resample['sf2'].values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Show all of the 100 results in faint yellow\n",
    "plt.plot(res_one['dt'], res_resample_arr.transpose(), alpha=0.3, color='yellow')\n",
    "plt.errorbar(res_one['dt'], res_one['sf2'], yerr=res_one['1_sigma'],\n",
    "             marker='.', capsize=2, label='single lightcurve result', color='orange')\n",
    "plt.plot(t_th, SF_th, marker='', label='theoretical result', \n",
    "         color='black', lw=2 )\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.title(\"Ensemble and single structure function\")\n",
    "plt.xlabel('Time [time unit]')\n",
    "plt.ylabel('SF^2 [magnitude unit^2]')\n",
    "plt.ylim(10**(-2), 10**(-0.4))\n",
    "plt.xlim(10**(1), 10**(3.8))\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us look at the distribution of results in the shortest time bin. With the orange line, we denote\n",
    "the result from the non-resampled lightcurve, and we compare the error reported from the calculation\n",
    "and the error computed ``manually'' above.\n",
    "\n",
    "We see quite a difference between the errors we computed manually and from the code! \n",
    "This problem is captured in: https://github.com/lincc-frameworks/tape/issues/198"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.hist(res_resample_arr.transpose()[0])\n",
    "err_manual = (np.quantile(res_resample_arr.transpose()[0], 0.84) -\n",
    "              np.quantile(res_resample_arr.transpose()[0], 0.16)) /2  \n",
    "plt.axvline(res_one['sf2'][0], color='orange', label='SF value')\n",
    "plt.axvline(res_one['sf2'][0]+res_one['1_sigma'][0], color='orange', ls='--',\n",
    "            label='reported error')\n",
    "plt.axvline(res_one['sf2'][0]-res_one['1_sigma'][0], color='orange', ls='--')\n",
    "plt.axvline(res_one['sf2'][0]+err_manual, color='orange', ls=':',\n",
    "            label='manually computed error')\n",
    "plt.axvline(res_one['sf2'][0]-err_manual, color='orange', ls=':')\n",
    "plt.legend()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Structure function methods"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Numerous different definitions exist for structure calculation. This is because the original\n",
    "formula is very sensitive to outliers (in a quadratic fashion), so a couple of outliers can\n",
    "completely distort the results. Therefore, multiple different approaches exist which produce the same \n",
    "result as the original equation under certain assumptions, but are less sensitive to problems in the data.\n",
    "\n",
    "To access these methods, specify the `sf_method` argument, as shown below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This takes at the moment 3 minutes, 25 seconds \n",
    "arg_container = tape.analysis.structure_function.StructureFunctionArgumentContainer()\n",
    "\n",
    "\n",
    "arg_container.sf_method = 'macleod_2012'\n",
    "arg_container.combine = True\n",
    "arg_container.bin_count_target = 100000\n",
    "arg_container.calculation_repetitions = 1\n",
    "res_macleod = ens.sf2( argument_container=arg_container)\n",
    "\n",
    "\n",
    "arg_container.sf_method = 'bauer_2009a'\n",
    "res_bauer_a = ens.sf2(argument_container=arg_container)\n",
    "\n",
    "arg_container.sf_method = 'bauer_2009b'\n",
    "res_bauer_b = ens.sf2(argument_container=arg_container)\n",
    "\n",
    "arg_container.sf_method = 'schmidt_2010'\n",
    "res_schmidt = ens.sf2(argument_container=arg_container)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plt.plot(res_basic['dt'], res_basic['sf2'], 'b', label='Basic', lw = 3, marker = 'o')\n",
    "plt.plot(res_macleod['dt'], res_macleod['sf2'], 'g',marker='.', label='MacLeod 2012')\n",
    "plt.plot(res_bauer_a['dt'], res_bauer_a['sf2'], 'magenta',marker='.', label='Bauer 2009a')\n",
    "plt.plot(res_bauer_b['dt'], res_bauer_b['sf2'], 'brown',marker='.', label='Bauer 2009b')\n",
    "plt.plot(res_schmidt['dt'], res_schmidt['sf2'], 'm',marker='.', label='Schmidt 2010')\n",
    "plt.plot(t_th, SF_th, marker='', label='theoretical result', \n",
    "         color='black', lw=2 )\n",
    "plt.legend()\n",
    "plt.yscale('log')\n",
    "plt.xscale('log')\n",
    "plt.xlabel('Time [time unit]')\n",
    "plt.ylabel('SF^2 [magnitude unit^2]')\n",
    "plt.ylim(10**(-2), 10**(-0.4))\n",
    "plt.xlim(10**(1), 10**(3.8))\n",
    "plt.legend()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.9"
  },
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
