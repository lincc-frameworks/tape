{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5ee0a705-b529-45cc-afb9-382350e63fb6",
   "metadata": {},
   "source": [
    "# PLAsTiCC data exploration with TAPE\n",
    "\n",
    "Let's explore [PLAsTiCC](http://plasticc.org) data!\n",
    "\n",
    "It is publically avilable through [this Zenodo repository](https://zenodo.org/record/2539456)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "## Uncomment to install packages\n",
    "\n",
    "# !pip install tape joblib requests"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T12:33:40.950351Z",
     "start_time": "2023-09-29T12:33:40.948610Z"
    }
   },
   "id": "a92caa3a0873700f"
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "%load_ext memory_profiler\n",
    "\n",
    "from pathlib import Path"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T13:46:18.984196Z",
     "start_time": "2023-09-29T13:46:18.954765Z"
    }
   },
   "id": "27d99d303e88f1e7"
  },
  {
   "cell_type": "markdown",
   "id": "8a8570b9-2a7b-4d79-be91-49a1d74d3f84",
   "metadata": {},
   "source": [
    "### Download the data\n",
    "\n",
    "Please get the data from [Zenodo](https://zenodo.org/record/2539456) and put them to the `./plasticc` folder (you may change the location bellow with `DATA_DIR`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"./plasticc\")\n",
    "\n",
    "META_FILENAME = \"plasticc_test_metadata.csv\"\n",
    "LC_FILENAMES = [f\"plasticc_test_lightcurves_{i:02d}.csv\" for i in range(1, 12)]\n",
    "# META_FILENAME = \"plasticc_train_metadata.csv.gz\"\n",
    "# LC_FILENAMES = ['plasticc_train_lightcurves.csv.gz']\n",
    "\n",
    "N_PARTITIONS = len(LC_FILENAMES)\n",
    "\n",
    "N_PROCESSORS = 4"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T13:46:20.059129Z",
     "start_time": "2023-09-29T13:46:20.054905Z"
    }
   },
   "id": "4866d31cc059f9d7"
  },
  {
   "cell_type": "markdown",
   "source": [
    "You may skip the next cell if you already have the data downloaded in the `DATA_DIR`."
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "f5cea8a584d359ad"
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Read and analyse the data with TAPE"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "61f1f943627a85da"
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading object table...\n",
      "Loading source tables...\n",
      "Building Ensemble...\n",
      "Starting analysis...\n",
      "First, filter by photoz\n",
      "Extract durations\n",
      "Assign a column\n",
      "Filter by duration\n",
      "Extract Otsu features\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-09-29 09:46:48,528 - distributed.worker.memory - WARNING - Unmanaged memory use is high. This may indicate a memory leak or the memory may not be released to the OS; see https://distributed.dask.org/en/latest/worker-memory.html#memory-not-released-back-to-the-os for more information. -- Unmanaged memory: 5.65 GiB -- Worker memory limit: 7.45 GiB\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Assign columns\n",
      "Filter by Otsu features\n",
      "Compute object table\n",
      "peak memory: 566.45 MiB, increment: 411.06 MiB\n",
      "CPU times: user 10.5 s, sys: 5.3 s, total: 15.8 s\n",
      "Wall time: 1min 53s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "import light_curve as licu\n",
    "import dask.dataframe as dd\n",
    "from tape import Ensemble, ColumnMapper\n",
    "\n",
    "# In TAPE's (and LSST's) terminology, sources are individual detections,\n",
    "# and objects are the underlying astrophysical objects.\n",
    "\n",
    "# We load object table first, from the metadata file.\n",
    "print(\"Loading object table...\")\n",
    "object_table = dd.read_csv(\n",
    "    DATA_DIR / META_FILENAME,\n",
    "    blocksize=100e6,\n",
    ")\n",
    "# object_table = object_table.set_index('object_id', sorted=True, sort=False,)# divisions=[13, 130788054])\n",
    "\n",
    "# Then we load the sources:\n",
    "print(\"Loading source tables...\")\n",
    "source_table = dd.read_csv(\n",
    "    [DATA_DIR / filename for filename in LC_FILENAMES],\n",
    "    blocksize=100e6,\n",
    ")\n",
    "# source_table = source_table.set_index('object_id', sorted=True, sort=False,) # divisions=[13, 1000183, 13952428, 26956806, 39933855, 52935297, 65930419, 78887012, 91875367, 104853940, 117792961, 130788054])\n",
    "\n",
    "# Now we can make an Ensemble.\n",
    "# To make parallel processing work we need to partition the data.\n",
    "# After that, when we run analysis, TAPE will distribute the work:\n",
    "# one partition per worker.\n",
    "print(\"Building Ensemble...\")\n",
    "# ens = Ensemble(dashboard_address=\"127.0.0.1:8787\", memory_limit='16GB')\n",
    "ens = Ensemble(\n",
    "    # sync_mode=False,\n",
    "    memory_limit='8GB',\n",
    "    n_workers=N_PROCESSORS,\n",
    ")\n",
    "ens.from_dask_dataframe(\n",
    "    source_frame=source_table,\n",
    "    object_frame=object_table,\n",
    "    # npartitions=N_PARTITIONS,\n",
    "    npartitions=None,\n",
    "    column_mapper=ColumnMapper(\n",
    "        id_col='object_id',\n",
    "        time_col='mjd',\n",
    "        flux_col='flux',\n",
    "        err_col='flux_err',\n",
    "        band_col='passband',\n",
    "    ),\n",
    "    sync_tables=False,\n",
    ")\n",
    "\n",
    "# Let's run some analysis!\n",
    "\n",
    "print(\"Starting analysis...\")\n",
    "# First, let's select only Galactic objects, by cutting on hostgal_photoz.\n",
    "print(\"First, filter by photoz\")\n",
    "ens = ens.query(\"hostgal_photoz < 1e-3\", table=\"object\")\n",
    "\n",
    "# Second, let's select persistent sources, by cutting on the duration of the light curve.\n",
    "print(\"Extract durations\")\n",
    "duration = ens.batch(\n",
    "    lambda time, detected: np.ptp(time[np.asarray(detected, dtype=bool)]),\n",
    "    ens._time_col, 'detected_bool',\n",
    "    meta=('duration', \"float64\"),\n",
    "    use_map=False,\n",
    "    compute=False,\n",
    ")\n",
    "print(\"Assign a column\")\n",
    "ens.assign(table=\"object\", duration=duration)\n",
    "print(\"Filter by duration\")\n",
    "ens = ens.query(\"duration > 366\", table=\"object\")\n",
    "\n",
    "# Next, we use Otsu's method to split light curves into two groups:\n",
    "# one with high flux, and one with low flux. Eclipsing binaries should have\n",
    "# lower flux group smaller than the higher flux group, but having larger \n",
    "# variability. We use light-curve package to extract these features.\n",
    "# (https://github.com/light-curve/light-curve-python)\n",
    "# For simplicity, we only calculate these features for the i band.\n",
    "print(\"Extract Otsu features\")\n",
    "otsu_features = ens.batch(licu.OtsuSplit(), band_to_calc=3, use_map=False, compute=False)\n",
    "print(\"Assign columns\")\n",
    "ens = ens.assign(\n",
    "    table=\"object\",\n",
    "    otsu_lower_to_all_ratio=otsu_features['otsu_lower_to_all_ratio'],\n",
    "    otsu_std_lower=otsu_features['otsu_std_lower'],\n",
    "    otsu_std_upper=otsu_features['otsu_std_upper'],\n",
    ")\n",
    "print('Filter by Otsu features')\n",
    "ens = ens.query(\n",
    "    \"otsu_lower_to_all_ratio < 0.1 and otsu_std_lower > otsu_std_upper\",\n",
    "    table=\"object\",\n",
    ")\n",
    "print(\"Compute object table\")\n",
    "ens.compute(\"object\")"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T13:48:15.986638Z",
     "start_time": "2023-09-29T13:46:22.076045Z"
    }
   },
   "id": "e023704e2d93eb13"
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Do the same, but with bare Pandas + PyArrow and nested arrays"
   ],
   "metadata": {
    "collapsed": false
   },
   "id": "c92b01b629070426"
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading object table...\n",
      "Loading source tables...\n",
      "Sanity checks...\n",
      "Updating object table with list-arrays...\n",
      "Starting analysis...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:119: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peak memory: 19421.34 MiB, increment: 18868.81 MiB\n",
      "CPU times: user 1min 41s, sys: 37 s, total: 2min 18s\n",
      "Wall time: 2min 21s\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<string>:156: UserWarning: Boolean Series key will be reindexed to match DataFrame index.\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%memit\n",
    "\n",
    "import light_curve as licu\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow as pa\n",
    "import pyarrow.csv as pacsv\n",
    "from joblib import Parallel, delayed\n",
    "\n",
    "# Read the data\n",
    "# -------------\n",
    "\n",
    "# First we load object table, from the metadata file.\n",
    "print(\"Loading object table...\")\n",
    "object_table = pacsv.read_csv(\n",
    "    DATA_DIR / META_FILENAME,\n",
    "    # We'd like to load the whole file into a single partition\n",
    "    read_options=pacsv.ReadOptions(block_size=(1<<31)-1),\n",
    ")\n",
    "object_table = pd.DataFrame(\n",
    "    {\n",
    "        col: pd.Series(\n",
    "            object_table[col],\n",
    "            dtype=pd.ArrowDtype(object_table[col].type),\n",
    "            index=object_table['object_id'],\n",
    "            copy=False,\n",
    "        )\n",
    "        for col in object_table.column_names if col != 'object_id'\n",
    "    },\n",
    ")\n",
    "\n",
    "# Then we load the sources:\n",
    "print(\"Loading source tables...\")\n",
    "\n",
    "def read_source_table(filename):\n",
    "    table = pacsv.read_csv(\n",
    "        DATA_DIR / filename,\n",
    "        # We'd like to have a partition per an original file\n",
    "        read_options=pacsv.ReadOptions(block_size=(1<<31)-1),\n",
    "    )\n",
    "    return pd.DataFrame(\n",
    "        {\n",
    "            col: pd.Series(\n",
    "                table[col],\n",
    "                dtype=pd.ArrowDtype(table[col].type),\n",
    "                index=table['object_id'],\n",
    "                copy=False,\n",
    "            )\n",
    "            for col in table.column_names if col != 'object_id'\n",
    "        },\n",
    "    )\n",
    "\n",
    "# source_tables = []\n",
    "# for filename in LC_FILENAMES:\n",
    "#     source_table = pa.csv.read_csv(\n",
    "#         DATA_DIR / filename,\n",
    "#         # We'd like to have a partition per an original file\n",
    "#         read_options=pa.csv.ReadOptions(block_size=(1<<31)-1),\n",
    "#     )\n",
    "#     source_tables.append(pd.DataFrame(\n",
    "#         {\n",
    "#             col: pd.Series(\n",
    "#                 source_table[col],\n",
    "#                 dtype=pd.ArrowDtype(source_table[col].type),\n",
    "#                 index=source_table['object_id'],\n",
    "#                 copy=False,\n",
    "#             )\n",
    "#             for col in source_table.column_names if col != 'object_id'\n",
    "#         },\n",
    "#     ))\n",
    "source_tables = Parallel(backend='threading', n_jobs=N_PROCESSORS)(\n",
    "    delayed(read_source_table)(filename) for filename in LC_FILENAMES\n",
    ")\n",
    "source_table = pd.concat(source_tables, ignore_index=True, sort=False)\n",
    "\n",
    "\n",
    "# Add sources to the object table\n",
    "# -------------------------------\n",
    "\n",
    "# First, let's do some sanity checks\n",
    "print(\"Sanity checks...\")\n",
    "np.testing.assert_array_equal(\n",
    "    object_table.index.values,\n",
    "    np.unique(object_table.index.values),\n",
    "    err_msg=\"Object table has duplicate indices or is not sorted.\",\n",
    ")\n",
    "assert np.all(np.diff(source_table.index) >= 0), \"Source table index must be sorted.\"\n",
    "\n",
    "# We need an offsets array to know where each source light curve starts.\n",
    "source_offsets = []\n",
    "for table in source_tables:\n",
    "    offset = np.nonzero(np.diff(table.index, prepend=-1, append=-1))[0]\n",
    "    source_offsets.append(pa.array(offset))\n",
    "\n",
    "# Update the object table with list-arrays built from the source table\n",
    "\n",
    "print(\"Updating object table with list-arrays...\")\n",
    "for column in source_table.columns:\n",
    "    list_arrays = []\n",
    "    for table, offset in zip(source_tables, source_offsets):\n",
    "        list_arrays.append(pa.ListArray.from_arrays(\n",
    "            offset,\n",
    "            pa.array(table[column]),\n",
    "        ))\n",
    "    chunked_array = pa.chunked_array(list_arrays)\n",
    "    object_table[column] = pd.Series(\n",
    "        chunked_array,\n",
    "        dtype=pd.ArrowDtype(chunked_array.type),\n",
    "        index=object_table.index,\n",
    "    )\n",
    "    \n",
    "# Do analysis\n",
    "# -----------\n",
    "\n",
    "print(\"Starting analysis...\")\n",
    "# First, let's select only Galactic objects, by cutting on hostgal_photoz.\n",
    "df = object_table[object_table['hostgal_photoz'] < 1e-3]\n",
    "\n",
    "# Second, let's select persistent sources, by cutting on the duration of the light curve.\n",
    "df['duration'] = df[['mjd', 'detected_bool']].apply(\n",
    "    lambda row: np.ptp(row['mjd'][np.asarray(row['detected_bool'], dtype=bool)]),\n",
    "    axis=1\n",
    ")\n",
    "df = df[df['duration'] > 366]\n",
    "\n",
    "# Next, we use Otsu's method to split light curves into two groups:\n",
    "# one with high flux, and one with low flux. Eclipsing binaries should have\n",
    "# lower flux group significantly smaller than the higher flux group,\n",
    "# but having larger variability.\n",
    "# We use light-curve package to extract these features.\n",
    "# (https://github.com/light-curve/light-curve-python)\n",
    "# For simplicity, we only calculate these features for the i band.\n",
    "def extract_band(*arrays, bands, band_to_calc):\n",
    "    mask = np.asarray(bands) == band_to_calc\n",
    "    return [np.asarray(arr)[mask] for arr in arrays]\n",
    "\n",
    "otsu_split = licu.OtsuSplit()\n",
    "otsu_features = df[['mjd', 'flux', 'flux_err', 'passband']].apply(\n",
    "    lambda row: pd.Series(\n",
    "        otsu_split(\n",
    "            *extract_band(\n",
    "                row['mjd'],\n",
    "                row['flux'],\n",
    "                row['flux_err'],\n",
    "                bands=row['passband'],\n",
    "                band_to_calc=3,  # i band\n",
    "            ),\n",
    "            sorted=True,\n",
    "            check=False,\n",
    "        ),\n",
    "        index=otsu_split.names,\n",
    "    ),\n",
    "    axis=1\n",
    ")\n",
    "\n",
    "df = df[otsu_features['otsu_lower_to_all_ratio'] < 0.1]\n",
    "df = df[otsu_features['otsu_std_lower'] > otsu_features['otsu_std_upper']]\n",
    "df"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-29T13:50:59.485388Z",
     "start_time": "2023-09-29T13:48:38.019632Z"
    }
   },
   "id": "9e215607e9fd17fd"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "LSST_BANDS = 'ugrizy'\n",
    "\n",
    "def plot(row):\n",
    "    plt.figure()\n",
    "    plt.title(f\"Object {row.Index}, true class {row.true_target}\")\n",
    "    plt.xlabel('MJD')\n",
    "    plt.ylabel('Flux, zp=27.5')\n",
    "    for band_idx, band_name in enumerate(LSST_BANDS):\n",
    "        mjd, flux, flux_err = extract_band(\n",
    "            row.mjd,\n",
    "            row.flux,\n",
    "            row.flux_err,\n",
    "            bands=row.passband,\n",
    "            band_to_calc=band_idx,\n",
    "        )\n",
    "        color = f'C{band_idx}'\n",
    "        plt.scatter(mjd, flux, c=color, label=band_name)\n",
    "        plt.errorbar(mjd, flux, yerr=flux_err, ls='none', c=color)\n",
    "        plt.legend()\n",
    "\n",
    "# Random objects from the selected sample\n",
    "for row in object_table[object_table['true_target'] == 16].sample(5, random_state=0).itertuples():\n",
    "    plot(row)\n",
    "    \n",
    "# Random objects from the selected sample\n",
    "for row in df.sample(5, random_state=0).itertuples():\n",
    "    plot(row)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "start_time": "2023-09-27T12:54:31.108198Z"
    }
   },
   "id": "1b336d01b8f74587"
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "7fe6c62b2e3df51e"
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "13           20468334592\n14           20468337232\n17           20468339872\n23           20468342688\n34           20468345328\n                ...     \n130787966    24097554064\n130787971    24097555224\n130787974    24097556344\n130788053    24097557480\n130788054    24097558304\nName: mjd, Length: 3492890, dtype: int64"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_table['mjd'].apply(lambda t: t.ctypes.data)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T20:52:45.230485Z",
     "start_time": "2023-09-07T20:52:39.758994Z"
    }
   },
   "id": "f2051fa7220567fc"
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "13           20468334592\n14           20468337232\n17           20468339872\n23           20468342688\n34           20468345328\n                ...     \n130787966    24097554064\n130787971    24097555224\n130787974    24097556344\n130788053    24097557480\n130788054    24097558304\nLength: 3492890, dtype: int64"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "object_table.apply(lambda row: row['mjd'].ctypes.data, axis=1)"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-06T20:43:09.636489Z",
     "start_time": "2023-09-06T20:42:14.143754Z"
    }
   },
   "id": "d7162734e034ba78"
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[330 330 352 ... 350 255 255]\n"
     ]
    },
    {
     "data": {
      "text/plain": "13             NaN\n14           330.0\n17           330.0\n23           352.0\n34           330.0\n             ...  \n130787966    139.0\n130787971    145.0\n130787974    140.0\n130788053    142.0\n130788054    103.0\nName: mjd, Length: 3492890, dtype: float64"
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(np.diff(pa.array(object_table['mjd']).chunks[0].offsets))\n",
    "object_table['mjd'].apply(lambda t: t.ctypes.data).diff() / 8"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2023-09-07T20:56:03.566747Z",
     "start_time": "2023-09-07T20:55:58.360624Z"
    }
   },
   "id": "e259be67d6991a76"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   },
   "id": "87bfa55fa6eedd45"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
