{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Binning Sources in an `Ensemble`"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this notebook we build upon the concepts introduced in the [\"Working with the tape Ensemble object\"](https://tape.readthedocs.io/en/latest/tutorials/working_with_the_ensemble.html)\n",
    " notebook to focus on internal helper functions for binning light curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tape.ensemble import Ensemble\n",
    "from tape.utils import ColumnMapper\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "ens = Ensemble()  # initialize an ensemble object\n",
    "\n",
    "# Read in data from a parquet file\n",
    "ens.from_parquet(\n",
    "    \"../../tests/tape_tests/data/source/test_source.parquet\",\n",
    "    id_col=\"ps1_objid\",\n",
    "    time_col=\"midPointTai\",\n",
    "    flux_col=\"psFlux\",\n",
    "    err_col=\"psFluxErr\",\n",
    "    band_col=\"filterName\",\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an `Ensemble` object filled with data from a parquet file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Depending on the data set that we use, our light curves could contain a large number of closely spaced observations. For example, we can see a histogram of the observations from the example parquet file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(ens.source[\"midPointTai\"].compute().tolist(), 500)\n",
    "ax.set_xlabel(\"Time (MJD)\")\n",
    "ax.set_ylabel(\"Source Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Binning Sources\n",
    "\n",
    "The `Ensemble` object provides the function `bin_sources()` to compress multiple sources down to a single observation. This can be useful for either a) reducing the noise in our source estimate by combining multiple sources, or b) reducing the amount of memory used to store sources. **Note**: Only slow changing sources should be combined so as not to lose valuable information about the changes themselves.\n",
    "\n",
    "The `bin_sources()` function uses floating-point truncation on the timestamp to produce the bins.  Specifically for a given timestamp `t`, window `time_window`, and `offset`, the bin is computed as:\n",
    "\n",
    "`b = np.floor((t + offset) / time_window) * time_window`\n",
    "\n",
    "The `time_window` parameter is specified in days and indicates how large the bins are. The `offset` parameter allows the user to indicate where the division between different nights/observing blocks should be. This should correspond to the time stamp's fractional part during the middle of the daylight hours.\n",
    "\n",
    "Below we use bin the results into one week buckets (`time_window=7.0`) and use `offset=0.0`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.bin_sources(time_window=7.0, offset=0.0)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(ens.source[\"midPointTai\"].compute().tolist(), 500)\n",
    "ax.set_xlabel(\"Time (MJD)\")\n",
    "ax.set_ylabel(\"Source Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By default `bin_sources` only preserves the id, band, timestamp, flux, and flux error columns. Timestamps and fluxes within a bin are computed as a simple (unweighted) *mean* value within the bin. The Flux error is computed by taking the `N` error estimates in the bin `[e_0, e_1, ... e_(N-1)]` as:\n",
    "\n",
    "`flux_error = np.sqrt(sum_i (e_i * e_i)) / N`\n",
    "\n",
    "We can include additional columns or use different aggregation functions by specifying them in the `custom_aggr` parameter. For example, if we want to use the minimum timestamp within a bin, we could set:\n",
    "\n",
    "```\n",
    "custom_aggr={'midPointTai':'min'}\n",
    "```\n",
    "\n",
    "**Note:** Using the custom aggregations will produce a warning if we overwrite the aggregation function for one of the main columns, such as `midPointTai` in this example."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.bin_sources(time_window=28.0, offset=0.0, custom_aggr={\"midPointTai\": \"min\"})\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(ens.source[\"midPointTai\"].compute().tolist(), 500)\n",
    "ax.set_xlabel(\"Time (MJD)\")\n",
    "ax.set_ylabel(\"Source Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Determining Offsets\n",
    "\n",
    "If we choose the wrong offset, we can end up splitting data from different nights while also splitting data from the same night. Consider the dataset below which represents two nights of observing where the fractional time 0.0 corresponds to midnight at the observatory. In this case timestamps 4.9 and 5.1 would represent observations on the same night."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = {\n",
    "    \"id\": [1, 1, 1, 1, 1, 1],\n",
    "    \"midPointTai\": [0.85, 0.9, 1.1, 4.9, 5.0, 5.1],\n",
    "    \"flux\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "    \"band\": [\"g\", \"g\", \"g\", \"g\", \"g\", \"g\"],\n",
    "}\n",
    "cmap = ColumnMapper(id_col=\"id\", time_col=\"midPointTai\", flux_col=\"flux\", err_col=\"err\", band_col=\"band\")\n",
    "ens.from_source_dict(rows, column_mapper=cmap)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(ens.source[\"midPointTai\"].compute().tolist(), 60)\n",
    "ax.set_xlabel(\"Time (MJD)\")\n",
    "ax.set_ylabel(\"Source Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If we bin with `offset=0.0`, we round each timestamp down to the nearest integer. This splits the observations within a night. The observation at time 4.9 is binned at 4.0 and the observation at 5.1 is binned at 5.0."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rows = {\n",
    "    \"id\": [1, 1, 1, 1, 1, 1],\n",
    "    \"midPointTai\": [0.85, 0.9, 1.1, 4.9, 5.0, 5.1],\n",
    "    \"flux\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "    \"band\": [\"g\", \"g\", \"g\", \"g\", \"g\", \"g\"],\n",
    "}\n",
    "cmap = ColumnMapper(id_col=\"id\", time_col=\"midPointTai\", flux_col=\"flux\", err_col=\"err\", band_col=\"band\")\n",
    "ens.from_source_dict(rows, column_mapper=cmap)\n",
    "ens.bin_sources(time_window=1.0, offset=0.0)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(ens.source[\"midPointTai\"].compute().tolist(), 60)\n",
    "ax.set_xlabel(\"Time (MJD)\")\n",
    "ax.set_ylabel(\"Source Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In contrast, if we bin at 0.5 (the middle of the day for this data), the observations on the same night are correctly associated."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reset the data.\n",
    "rows = {\n",
    "    \"id\": [1, 1, 1, 1, 1, 1],\n",
    "    \"midPointTai\": [0.85, 0.9, 1.1, 4.9, 5.0, 5.1],\n",
    "    \"flux\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "    \"band\": [\"g\", \"g\", \"g\", \"g\", \"g\", \"g\"],\n",
    "}\n",
    "cmap = ColumnMapper(id_col=\"id\", time_col=\"midPointTai\", flux_col=\"flux\", err_col=\"err\", band_col=\"band\")\n",
    "ens.from_source_dict(rows, column_mapper=cmap)\n",
    "ens.bin_sources(time_window=1.0, offset=0.5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(ens.source[\"midPointTai\"].compute().tolist(), 60)\n",
    "ax.set_xlabel(\"Time (MJD)\")\n",
    "ax.set_ylabel(\"Source Count\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The optimal offset is the middle of the daylight hours for the timezone where the observatory is located. Ideally we compute this directly from our knowledge of the observatory's longitude.\n",
    "\n",
    "We also provide a helper function that tries to estimate this offset from the data. `find_day_gap_offset()` computes a histogram of the number of observations in each hour (24 bins). It aggregates these counts over all days, so telescope downtime on one night (due to maintenance or weather) would be offset by observations from another night. A bin will only end up with zero if the data never contains a source during this hour of the day. \n",
    "\n",
    "The code looks for the longest stretch of bins with zero counts. It returns the middle timestamp of this stretch as the proposed offset.\n",
    "\n",
    "There are a few important notes when using `find_day_gap_offset`:\n",
    "* The function uses a Dask `compute()` to materialize the histogram and thus does not use lazy evaluation. As discussed in the other notebooks, one of the advantages of Dask is that it uses lazy evaluation to hold off computations until they are needed, allowing the work to be batched. Because we need to access values in the histogram, this function forces the computation.\n",
    "* The function is designed for observatories in a single (or small range of) timezones. If the data combines observations throughout the entire 24 hour window, there is not “day time” to use."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Reload the data from the parquet file (to undo the previous binning)\n",
    "ens.from_parquet(\n",
    "    \"../../tests/tape_tests/data/source/test_source.parquet\",\n",
    "    id_col=\"ps1_objid\",\n",
    "    time_col=\"midPointTai\",\n",
    "    flux_col=\"psFlux\",\n",
    "    err_col=\"psFluxErr\",\n",
    "    band_col=\"filterName\",\n",
    ")\n",
    "suggested_offset = ens.find_day_gap_offset()\n",
    "print(f\"Suggested offset is {suggested_offset}\")\n",
    "\n",
    "# Reset the data and plot with the suggested offset.\n",
    "rows = {\n",
    "    \"id\": [1, 1, 1, 1, 1, 1],\n",
    "    \"midPointTai\": [0.85, 0.9, 1.1, 4.9, 5.0, 5.1],\n",
    "    \"flux\": [0.0, 1.0, 2.0, 3.0, 4.0, 5.0],\n",
    "    \"band\": [\"g\", \"g\", \"g\", \"g\", \"g\", \"g\"],\n",
    "}\n",
    "cmap = ColumnMapper(id_col=\"id\", time_col=\"midPointTai\", flux_col=\"flux\", err_col=\"err\", band_col=\"band\")\n",
    "ens.from_source_dict(rows, column_mapper=cmap)\n",
    "ens.bin_sources(time_window=1.0, offset=0.5)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "ax.hist(ens.source[\"midPointTai\"].compute().tolist(), 60)\n",
    "ax.set_xlabel(\"Time (MJD)\")\n",
    "ax.set_ylabel(\"Source Count\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "vscode": {
   "interpreter": {
    "hash": "08968836a6367873274ed1d5e98a07391f42fc3a62bd5aba54afbd7b11ba8673"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
