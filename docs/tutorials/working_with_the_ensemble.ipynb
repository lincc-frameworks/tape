{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Working with the TAPE `Ensemble` object"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When working with many lightcurves, the TAPE `Ensemble` object serves as a singular interface for storing, filtering, and analyzing timeseries data. \n",
    "Let's consider an example set of lightcurves, generated as follows:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "np.random.seed(1)    \n",
    "\n",
    "# Generate 10 astronomical objects\n",
    "n_obj = 10\n",
    "ids = 8000 + np.arange(n_obj)\n",
    "names = ids.astype(str)\n",
    "object_table = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": ids, \n",
    "        \"name\": names,\n",
    "        \"ddf_bool\": np.random.randint(0, 2, n_obj), # 0 if from deep drilling field, 1 otherwise\n",
    "        \"libid_cadence\": np.random.randint(1, 130, n_obj),\n",
    "    }\n",
    ")\n",
    "\n",
    "# Create 1000 lightcurves with 100 measurements each\n",
    "lc_len = 100\n",
    "num_points = 1000\n",
    "all_bands = np.array([\"r\", \"g\", \"b\", \"i\"])\n",
    "source_table = pd.DataFrame(\n",
    "    {\n",
    "        \"id\": 8000 + (np.arange(num_points) % n_obj),\n",
    "        \"time\": np.arange(num_points),\n",
    "        \"flux\":  np.random.random_sample(size=num_points)*10,\n",
    "        \"band\": np.repeat(all_bands, num_points / len(all_bands)),\n",
    "        \"error\": np.random.random_sample(size=num_points),\n",
    "        \"count\": np.arange(num_points),\n",
    "    },\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can load these into the `Ensemble` using `Ensemble.from_pandas()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:36.125402Z",
     "start_time": "2023-08-30T14:58:34.190790Z"
    }
   },
   "outputs": [],
   "source": [
    "from tape.ensemble import Ensemble\n",
    "\n",
    "ens = Ensemble()  # initialize an ensemble object\n",
    "\n",
    "# Read in the generated lightcurve data\n",
    "ens.from_pandas(\n",
    "    source_frame=source_table,\n",
    "    object_frame=object_table,\n",
    "    id_col=\"id\",\n",
    "    time_col=\"time\",\n",
    "    flux_col=\"flux\",\n",
    "    err_col=\"error\",\n",
    "    band_col=\"band\",\n",
    "    npartitions=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We now have an `Ensemble` object, and have provided it with the constructed data in the source dictionary. Within the call to `Ensemble.from_pandas`, we specified which columns of the input file mapped to timeseries quantities that the `Ensemble` needs to understand. It's important to link these arguments properly, as the `Ensemble` will use these columns when operations are requested on understood quantities. For example, if a TAPE analysis function requires the time column, from this linking the `Ensemble` will automatically supply that function with the 'time' column."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Column Mapping with the ColumnMapper\n",
    "\n",
    "In the above example, we manually provide the column labels within the call to `Ensemble.from_pandas`. Alternatively, the `tape.utils.ColumnMapper` class offers a means to assign the column mappings. Either manually as shown before, or even populated from a known mapping scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:36.209050Z",
     "start_time": "2023-08-30T14:58:36.115521Z"
    }
   },
   "outputs": [],
   "source": [
    "from tape.utils import ColumnMapper\n",
    "\n",
    "# columns assigned manually\n",
    "col_map = ColumnMapper().assign(id_col=\"id\",\n",
    "                                time_col=\"time\",\n",
    "                                flux_col=\"flux\",\n",
    "                                err_col=\"error\",\n",
    "                                band_col=\"band\")\n",
    "\n",
    "# Pass the ColumnMapper along to from_pandas\n",
    "ens.from_pandas(\n",
    "    source_frame=source_table,\n",
    "    object_frame=object_table,\n",
    "    column_mapper=col_map,\n",
    "    npartitions=1)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Object and Source Frames\n",
    "The `Ensemble` maintains two dataframes under the hood, the \"object dataframe\" and the \"source dataframe\". This borrows from the Rubin Observatories object-source convention, where object denotes a given astronomical object and source is the collection of measurements of that object. Essentially, the Object frame stores one-off information about objects, and the source frame stores the available time-domain data. As a result, `Ensemble` functions that operate on the underlying dataframes need to be pointed at either object or source. In most cases, the default is the object table as it's a more helpful interface for understanding the contents of the `Ensemble`, especially when dealing with large volumes of data.\n",
    "\n",
    "We can also access Ensemble frames individually with `Ensemble.source` and `Ensemble.object`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dask and \"Lazy Evaluation\"\n",
    "\n",
    "Before going any further, the `Ensemble` is built on top of `Dask`, which brings with it a powerful framework for parallelization and scalability. However, there are some differences in how `Dask` code works that, if you're unfamiliar with it, is worth establishing right here at the get-go. The first is that `Dask` evaluates code \"lazily\". Meaning that many operations are not executed when the line of code is run, but instead are added to a scheduler to be executed when the result is actually needed. See below for an example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:36.219081Z",
     "start_time": "2023-08-30T14:58:36.205629Z"
    }
   },
   "outputs": [],
   "source": [
    "ens.source  # We have not actually loaded any data into memory"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are accessing the Dask dataframe and despite running a command to read in our source data, we only see an empty dataframe with some high-level information available. To explicitly bring the data into memory, we must run a `compute()` command on the data's frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:36.484627Z",
     "start_time": "2023-08-30T14:58:36.213215Z"
    }
   },
   "outputs": [],
   "source": [
    "ens.source.compute()  # Compute lets dask know we're ready to bring the data into memory"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With this compute, we see above that we have returned a populated dataframe (a Pandas dataframe in fact!). From this, many workflows in Dask and by extension TAPE, will look like a series of lazily evaluated commands that are chained together and then executed with a .compute() call at the end of the workflow.\n",
    "\n",
    "Alternatively we can use `ens.persist()` to execute the chained commands without loading the result into memory. This can speed up future `compute()` calls.\n",
    "\n",
    "Note that `Ensemble.source` and `Ensemble.object` are instances of the `tape.SourceFrame` and `tape.ObjectFrame` classes respectively. These are subclasses of Dask dataframes that provide some additional utility for tracking by the ensemble while supporting most of the Dask dataframe API.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Inspection, Filtering, and Selecting\n",
    "\n",
    "The `Ensemble` contains an assortment of functions for inspecting and filtering your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Inspection\n",
    "\n",
    "These functions provide views into the contents of your `Ensemble` dataframe, especially important when dealing with large data volumes that cannot be brought into memory all at once. The first is `Ensemble.info` which provides information on the columns, data types, and memory usage of the dataframe."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:36.696142Z",
     "start_time": "2023-08-30T14:58:36.361967Z"
    }
   },
   "outputs": [],
   "source": [
    "# Inspection\n",
    "\n",
    "ens.info(verbose=True, memory_usage=True)  # Grabs high level information about the dataframes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Ensemble.info` shows that we have 2000 rows and the the memory they use, and it also shows the columns we've brought in with their respective data types. If you'd like to actually bring a few rows into memory to inspect, `EnsembleFrame.head` and `EnsembleFrame.tail` provide access to the first n and last n rows respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:36.696879Z",
     "start_time": "2023-08-30T14:58:36.510953Z"
    }
   },
   "outputs": [],
   "source": [
    "ens.object.head(5)  # Grabs the first 5 rows of the object table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:36.697259Z",
     "start_time": "2023-08-30T14:58:36.561399Z"
    }
   },
   "outputs": [],
   "source": [
    "ens.source.tail(5)  # Grabs the last 5 rows of the source table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, when you are working with a small enough dataset, `Ensemble.compute` can be used to bring the whole dataframe into memory (as shown previously). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:36.697769Z",
     "start_time": "2023-08-30T14:58:36.592238Z"
    }
   },
   "outputs": [],
   "source": [
    "ens.source.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filtering\n",
    "\n",
    "The `Ensemble` provides a general filtering function [`query`](https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.query.html) that mirrors a Pandas or Dask `query` command. Specifically, the function takes a string that provides an expression indicating which rows to **keep**. As with other `Ensemble` functions, an optional `table` parameter allows you to filter on either the object or the source table.\n",
    "\n",
    "For example, the following code filters the sources to only include rows with flux values above the median. It uses `ens._flux_col` to retrieve the name of the column with that information."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:36.698305Z",
     "start_time": "2023-08-30T14:58:36.615492Z"
    }
   },
   "outputs": [],
   "source": [
    "highest_flux = ens.source[ens._flux_col].quantile(0.95).compute()\n",
    "ens.source.query(f\"{ens._flux_col} < {highest_flux}\").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Alternatively, we could use a Dask dataseries of Booleans to indicate which rows to *keep*. We can often compute these series as the result of some operation on the underlying tables:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:36.754980Z",
     "start_time": "2023-08-30T14:58:36.669055Z"
    }
   },
   "outputs": [],
   "source": [
    "# Find all of the source points with the lowest 90% of errors.\n",
    "keep_rows = ens.source[\"error\"] < ens.source[\"error\"].quantile(0.9)\n",
    "keep_rows.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We also provide filtering at the `Ensemble` level, so you can pass the above series to the `Ensemble.filter_from_series` function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:36.792088Z",
     "start_time": "2023-08-30T14:58:36.690772Z"
    }
   },
   "outputs": [],
   "source": [
    "ens.filter_from_series(keep_rows, table=\"source\")\n",
    "ens.source.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Additionally, several more specific functions are available for common operations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:37.026887Z",
     "start_time": "2023-08-30T14:58:36.715537Z"
    }
   },
   "outputs": [],
   "source": [
    "# Cleaning nans\n",
    "ens.source.dropna()  # clean nans from source table\n",
    "ens.object.dropna()  # clean nans from object table\n",
    "\n",
    "# Filtering on number of observations\n",
    "ens.prune(threshold=10)  # threshold is the minimum number of observations needed to retain the object\n",
    "\n",
    "ens.info(verbose=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the above operations, we remove any rows that have at least 1 NaN value present. And then filter such that only lightcurves which have at least 50 measurements are retained."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Selecting\n",
    "\n",
    "The `Ensemble` also provides a `select` function to filter down to a subset of columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:37.095991Z",
     "start_time": "2023-08-30T14:58:36.917820Z"
    }
   },
   "outputs": [],
   "source": [
    "# Add a new column so we can filter it out later.\n",
    "ens.source.assign(band2=ens.source[\"band\"] + \"2\").compute()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:37.096860Z",
     "start_time": "2023-08-30T14:58:36.937579Z"
    }
   },
   "outputs": [],
   "source": [
    "ens.select([\"time\", \"flux\", \"error\", \"band\"], table=\"source\")\n",
    "print(\"The Source table is dirty: \" + str(ens.source.is_dirty()))\n",
    "ens.source.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Updating an Ensemble's Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Ensemble` is a manager of `EnsembleFrame` objects (of which `Ensemble.source` and `Ensemble.object` are special cases). When performing operations on one of the tables, the results are not automatically sent to the `Ensemble`.\n",
    "\n",
    "So while in the above examples we demonstrate several methods where we generated filtered views of the source table, note that the underlying data remained unchanged, with no changes to the rows or columns of `Ensemble.source`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queried_src = ens.source.query(f\"{ens._flux_col} < {highest_flux}\")\n",
    "\n",
    "print(len(queried_src))\n",
    "print(len(ens.source))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "When modifying the views of a dataframe tracked by the `Ensemble`, we can update the `Source` or `Object` frame to use the updated view by calling\n",
    "\n",
    "`Ensemble.update_frame(view_frame)`\n",
    "\n",
    "Or alternately:\n",
    "\n",
    "`view_frame.update_ensemble()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now apply the views filter to the source frame.\n",
    "queried_src.update_ensemble()\n",
    "\n",
    "ens.source.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that the above is still a series of lazy operations that will not be fully evaluated until an operation such as `compute`. So a call to `update_ensemble` will not yet alter or move any underlying data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Assignments and Column Manipulation\n",
    "\n",
    "The ensemble object supports assignment through the Dask `assign` function. We can pass in either a callable or a series to assign to the new column. New column names are produced automatically from the argument name.\n",
    "\n",
    "For example, if we want to compute the lower bound of an error range as the estimated flux minus twice the estimated error, we would use:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:37.097571Z",
     "start_time": "2023-08-30T14:58:36.958927Z"
    }
   },
   "outputs": [],
   "source": [
    "lower_bnd = ens.source.assign(lower_bnd=lambda x: x[\"flux\"] - 2.0 * x[\"error\"])\n",
    "lower_bnd"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Batch Analysis\n",
    "\n",
    "The `Ensemble` provides a powerful batching interface, `Ensemble.batch`, with in-built parallelization (provided the input data is in multiple partitions). In addition, TAPE has a suite of analysis functions on-hand for your use. Below, we show the application of `tape.analysis.calc_stetson_J` on our dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:37.492980Z",
     "start_time": "2023-08-30T14:58:36.981314Z"
    }
   },
   "outputs": [],
   "source": [
    "# using tape analysis functions\n",
    "from tape.analysis import calc_stetson_J\n",
    "\n",
    "res = ens.batch(calc_stetson_J, compute=True)  # compute is set to true to execute immediately (non-lazily)\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Storing and Accessing Result Frames"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note for the above `batch` operation, we also printed:\n",
    "\n",
    "`Using generated label, result_1, for a batch result.`\n",
    "\n",
    "In addition to the source and object frames, the `Ensemble` may track other frames as well, accessed by either generated or user-provided labels.\n",
    "\n",
    "We can access a saved frame with `Ensemble.select_frame(label)`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.select_frame(\"result_1\").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`Ensemble.batch` has an optional `label` argument that will store the result with a user-provided label."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "res = ens.batch(calc_stetson_J, compute=True, label=\"stetson_j\")\n",
    "\n",
    "ens.select_frame(\"stetson_j\").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Likewise we can rename a frame with with a new label, and drop the original frame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.add_frame(ens.select_frame(\"stetson_j\"), \"stetson_j_result_1\") # Add result under new label\n",
    "ens.drop_frame(\"stetson_j\") # Drop original label\n",
    "\n",
    "ens.select_frame(\"stetson_j_result_1\").compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also add our own frames with `Ensemble.add_frame(frame, label)`. For instance, we can copy this result and add it to a new frame for the `Ensemble` to track as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.add_frame(res.copy(), \"new_res\")\n",
    "ens.select_frame(\"new_res\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Finally we can also drop frames we are no longer interested in having the `Ensemble` track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ens.drop_frame(\"result_1\")\n",
    "\n",
    "try:\n",
    "    ens.select_frame(\"result_1\") # This should result in a KeyError since the frame has been dropped.\n",
    "except Exception as e:\n",
    "    print(\"As expected, the frame 'result_1 was dropped.\\n\" + str(e))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Keeping the Object and Source Tables in Sync\n",
    "\n",
    "The Tape `Ensemble` attempts to lazily \"sync\" the Object and Source tables such that:\n",
    "\n",
    "* If a series of operations removes all lightcurves for a particular object from the Source table, we will lazily remove that object from the Object table.\n",
    "* If a series of operations removes an object from the Object table, we will lazily remove all light curves for that object from the Source table.\n",
    "\n",
    "As an example let's filter the Object table only for objects observed from deep drilling fields. This operation marks the result table as `dirty` indicating to the `Ensemble` that if used as part of a result computation, it should check if the object and source tables are synced. \n",
    "\n",
    "Note that because we have not called `update_ensemble()` the `Ensemble` is still using the original Object table which is **not** marked `dirty`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_only = ens.object.query(\"ddf_bool == True\")\n",
    "\n",
    "print(\"Object table is dirty: \" + str(ens.object.is_dirty()))\n",
    "print(\"ddf_only is dirty: \" + str(ddf_only.is_dirty()))\n",
    "ddf_only.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's update the `Ensemble`'s Object table. We can see that the Object table is now considered \"dirty\" so a sync between the Source and Object tables will be triggered by computing a `batch` operation. \n",
    "\n",
    "As part of the sync the Source table has been modified to drop all sources for objects not observed via Deep Drilling Fields. This is reflected both in the `batch` result output and in the reduced number of rows in the Source table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ddf_only.update_ensemble()\n",
    "print(\"Updated object table is now dirty: \" + str(ens.object.is_dirty()))\n",
    "\n",
    "print(\"Length of the Source table before the batch operation: \" + str(len(ens.source)))\n",
    "res = ens.batch(calc_stetson_J, compute=True)\n",
    "print(\"Post-computation object table is now dirty: \" + str(ens.object.is_dirty()))\n",
    "print(\"Length of the Source table after the batch operation: \" + str(len(ens.source)))\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To summarize:\n",
    "\n",
    "* An operation that alters a frame marks that frame as \"dirty\"\n",
    "* Such an operation on `Ensemble.source` or `Ensemble.object` won't cause a sync unless the output frame is stored back to either `Ensemble.source` or `Ensemble.object` respectively. This is usually done by a call to `EnsembleFrame.update_ensemble()`\n",
    "* Syncs are done lazily such that even when the Object and/or Source frames are \"dirty\", a sync between tables won't be triggered until a relevant computation yields an observable output, such as `batch(..., compute=True)`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "## Using light-curve package features\n",
    "\n",
    "`Ensemble.batch` also supports the use of [light-curve](https://pypi.org/project/light-curve/) package feature extractor:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:37.514514Z",
     "start_time": "2023-08-30T14:58:37.494001Z"
    }
   },
   "outputs": [],
   "source": [
    "import light_curve as licu\n",
    "\n",
    "extractor = licu.Extractor(licu.Amplitude(), licu.AndersonDarlingNormal(), licu.StetsonK())\n",
    "res = ens.batch(extractor, compute=True, band_to_calc=\"g\")\n",
    "res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Using a Custom Analysis Function\n",
    "The analysis functions contained in TAPE are meant to provide a collection of performant, on-hand routines for common timeseries use cases. However, TAPE is also equipped to handle externally defined functions. Let's walk through a short example of defining a simple custom function and applying it through `Ensemble.batch`.\n",
    "\n",
    "Here we define a simple function, that returns an average flux for each photometric band. It requires an array of fluxes, an array of band labels per measurement, and has a keyword argument for determining which averaging strategy to use (mean or median)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:37.519972Z",
     "start_time": "2023-08-30T14:58:37.515404Z"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "\n",
    "# Defining a simple function\n",
    "def my_flux_average(flux_array, band_array, method=\"mean\"):\n",
    "    \"\"\"Read in an array of fluxes, and return the average of the fluxes by band\"\"\"\n",
    "    res = {}\n",
    "    for band in np.unique(band_array):\n",
    "        mask = [band_array == band]  # Create a band by band mask\n",
    "        band_fluxes = flux_array[tuple(mask)]  # Mask the flux array\n",
    "        if method == \"mean\":\n",
    "            res[band] = np.mean(band_fluxes)\n",
    "        elif method == \"median\":\n",
    "            res[band] = np.median(band_fluxes)\n",
    "    return res"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With the function defined, we next supply it to `Ensemble.batch`. The column labels of the `Ensemble` columns we want to use as arguments must be provided, as well as any keyword arguments. In this case, we pass along `\"flux\"` and `\"band\"`, so that the `Ensemble` will map those columns to `flux_array` and `band_array` respectively. We also pass `method='mean'`, which will pass that kwarg along to `my_flux_average`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:37.583850Z",
     "start_time": "2023-08-30T14:58:37.519056Z"
    }
   },
   "outputs": [],
   "source": [
    "# Applying the function to the ensemble\n",
    "res = ens.batch(my_flux_average, \"flux\", \"band\", compute=True, meta=None, method=\"median\")\n",
    "res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We see that we now have a `Pandas.series` of `my_average_flux` result by object_id (lightcurve). In many cases, this may not be the ideal output for your function. This output is controlled by the `Dask` `meta` parameter. For more information on this parameter, you can read the `Dask` [documentation](https://blog.dask.org/2022/08/09/understanding-meta-keyword-argument). You may pass the `meta` parameter through `Ensemble.batch`, as shown above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-08-30T14:58:37.764841Z",
     "start_time": "2023-08-30T14:58:37.539014Z"
    }
   },
   "outputs": [],
   "source": [
    "ens.client.close()  # Tear down the ensemble client"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "08968836a6367873274ed1d5e98a07391f42fc3a62bd5aba54afbd7b11ba8673"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
